{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1719, 33),\n",
       " (430, 33),\n",
       " '/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/train_forgetfulness.csv',\n",
       " '/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/test_forgetfulness.csv')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# æœ¬åœ°è·¯å¾„\n",
    "data_path = \"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_disease_data.csv\"\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# æ£€æŸ¥å‰å‡ è¡Œ\n",
    "df.head()\n",
    "\n",
    "# ç§»é™¤ä¸éœ€è¦çš„åˆ—\n",
    "df_clean = df.drop(columns=[\"PatientID\", \"DoctorInCharge\"])\n",
    "\n",
    "# è®¾ç½®ç¬¬ä¸€é˜¶æ®µçš„ç›®æ ‡å˜é‡ï¼Œä¾‹å¦‚é¢„æµ‹ Forgetfulness ç—‡çŠ¶\n",
    "target_variable = \"Forgetfulness\"\n",
    "\n",
    "# æ‹†åˆ†ç‰¹å¾å’Œæ ‡ç­¾\n",
    "X = df_clean.drop(columns=[target_variable])\n",
    "y = df_clean[target_variable]\n",
    "\n",
    "# 80% è®­ç»ƒé›†ï¼Œ20% æµ‹è¯•é›†ï¼Œä¿æŒæ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ä¸€è‡´\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾ï¼Œä¿å­˜ä¸º DataFrame\n",
    "train_df = X_train.copy()\n",
    "train_df[target_variable] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df[target_variable] = y_test\n",
    "\n",
    "# å±•ç¤ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†å¤§å°\n",
    "import os\n",
    "output_dir = \"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets\"\n",
    "train_path = os.path.join(output_dir, \"train_forgetfulness.csv\")\n",
    "test_path = os.path.join(output_dir, \"test_forgetfulness.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "(train_df.shape, test_df.shape, train_path, test_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.85-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from biopython) (2.2.5)\n",
      "Downloading biopython-1.85-cp311-cp311-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: biopython\n",
      "Successfully installed biopython-1.85\n"
     ]
    }
   ],
   "source": [
    "! pip install biopython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰¾åˆ°æ–‡çŒ®æ•°é‡: 2000\n",
      "å·²ä¿å­˜ 1985 ç¯‡æ‘˜è¦åˆ° csv æ–‡ä»¶ï¼š/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "# è®¾ç½®é‚®ç®±ï¼ˆNCBI è¦æ±‚ï¼‰\n",
    "Entrez.email = \"annabian1122@gmail.com\"\n",
    "\n",
    "# ç¬¬ä¸€æ­¥ï¼šç”¨å…³é”®è¯æœç´¢æ–‡ç« ï¼ˆæ¯”å¦‚ Alzheimer'sï¼‰\n",
    "search_term = \"Alzheimer's disease AND (symptoms OR diagnosis) AND English[lang]\"\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=2000)\n",
    "record = Entrez.read(handle)\n",
    "handle.close()\n",
    "\n",
    "# è·å– PubMed ID åˆ—è¡¨\n",
    "id_list = record[\"IdList\"]\n",
    "print(f\"æ‰¾åˆ°æ–‡çŒ®æ•°é‡: {len(id_list)}\")\n",
    "\n",
    "# æ›´å¹²å‡€çš„æ–¹å¼ï¼šç”¨ XML æ ¼å¼è·å–æ‘˜è¦æ•°æ®\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_list), rettype=\"abstract\", retmode=\"xml\")\n",
    "records = Entrez.read(handle)\n",
    "handle.close()\n",
    "\n",
    "abstracts = []\n",
    "pmids = []\n",
    "\n",
    "for article in records['PubmedArticle']:\n",
    "    try:\n",
    "        abstract_text = article['MedlineCitation']['Article']['Abstract']['AbstractText']\n",
    "        # å¤„ç†æˆçº¯å­—ç¬¦ä¸²\n",
    "        abstract_str = \" \".join(abstract_text)\n",
    "        pmid = article['MedlineCitation']['PMID']\n",
    "        abstracts.append(abstract_str)\n",
    "        pmids.append(pmid)\n",
    "    except:\n",
    "        # æ²¡æœ‰æ‘˜è¦å°±è·³è¿‡\n",
    "        continue\n",
    "\n",
    "# ä¿å­˜æˆ DataFrameï¼ˆç¡®ä¿åªåŒ…å«æœ‰æ‘˜è¦çš„æ–‡ç« ï¼‰\n",
    "df = pd.DataFrame({\n",
    "    \"PMID\": pmids,\n",
    "    \"Abstract\": abstracts\n",
    "})\n",
    "\n",
    "\n",
    "# ä¿å­˜ä¸º CSV æ–‡ä»¶ï¼ˆæŒ‡å®šè·¯å¾„ï¼‰\n",
    "csv_path = \"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"å·²ä¿å­˜ {len(df)} ç¯‡æ‘˜è¦åˆ° csv æ–‡ä»¶ï¼š{csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(id_list): 2000\n",
      "len(abstracts): 1985\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(id_list): {len(id_list)}\")\n",
    "print(f\"len(abstracts): {len(abstracts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å®Œæˆå…³é”®è¯æ ‡ç­¾æ‰“æ ‡ï¼Œä¿å­˜æ–°æ–‡ä»¶ã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¯»å–ä½ æ¸…æ´—ä¿å­˜çš„æ‘˜è¦æ•°æ®\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\")\n",
    "\n",
    "# è‡ªå®šä¹‰ç—‡çŠ¶å…³é”®è¯åˆ—è¡¨ï¼ˆå¯æ‰©å±•ï¼‰\n",
    "symptom_keywords = [\n",
    "    \"memory loss\", \"forgetfulness\", \"cognitive decline\", \"confusion\",\n",
    "    \"disorientation\", \"language impairment\", \"attention deficit\", \"behavioral change\",\n",
    "    \"mild cognitive impairment\", \"cognitive symptoms\"\n",
    "]\n",
    "\n",
    "# æ·»åŠ æ–°åˆ— MentionsSymptomï¼šæ˜¯å¦åŒ…å«ä»»ä¸€å…³é”®è¯ï¼ˆ1/0ï¼‰\n",
    "df[\"MentionsSymptom\"] = df[\"Abstract\"].apply(\n",
    "    lambda text: int(any(kw.lower() in text.lower() for kw in symptom_keywords))\n",
    ")\n",
    "\n",
    "# ä¿å­˜ä¸ºæ–°æ–‡ä»¶\n",
    "df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts_labeled.csv\", index=False)\n",
    "\n",
    "print(\"âœ… å·²å®Œæˆå…³é”®è¯æ ‡ç­¾æ‰“æ ‡ï¼Œä¿å­˜æ–°æ–‡ä»¶ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.2.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# å®‰è£… spaCy\n",
    "! pip install spacy\n",
    "\n",
    "# ä¸‹è½½è‹±æ–‡å°æ¨¡å‹\n",
    "! python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª æµ‹è¯•å¥å­å®ä½“è¯†åˆ«:\n",
      " - memory loss (SYMPTOM)\n",
      " - confusion (SYMPTOM)\n",
      "\n",
      "ğŸ“‹ æ‰¹é‡å¤„ç†æ‘˜è¦ä¸­çš„å®ä½“è¯†åˆ«ç»“æœï¼š\n",
      "\n",
      "ğŸ“„ Abstract #1:\n",
      "Lesion-symptom mapping methods assess the relationship between lesions caused by cerebral small vessel disease and cognition, but current technology like support vector regression (SVR)) primarily provide group-level results. We propose a novel lesion-symptom mapping approach that can indicate how lesion patterns contribute to cognitive impairment on an individual level. A convolutional neural network (CNN) predicts cognitive scores and is combined with explainable artificial intelligence (XAI) to map the relation between cognition and vascular lesions. This method was evaluated primarily using real white matter hyperintensity maps of 821 memory clinic patients and simulated cognitive data, with weighted lesions and noise levels. Simulated data provided ground truth locations to assess predictive performance of the CNN and accuracy of strategic lesion identification by XAI, using an established lesion-symptom mapping method, SVR, and a simple fully connected neural network (FNN) as benchmarks. Real cognitive scores were used in a final proof-of-principle analysis. Predictive performance in simulation experiments was high for the CNN (R<sup>2</sup>Â =Â 0.964), SVR (R<sup>2</sup>Â =Â 0.875), and FNN (R<sup>2</sup>Â =Â 0.863). CNN with XAI provided patient-specific attribution maps that highlighted the ground truth locations. All methods showed similar sensitivity to noise. Using real cognitive scores, SVR (R<sup>2</sup>Â =Â 0.291) obtained a somewhat higher predictive performance than the CNN (R<sup>2</sup>Â =Â 0.216), although both methods substantially exceeded the predictive performance of total WMH volume alone (R<sup>2</sup>Â =Â 0.013). The FNN performed worse on real data (R<sup>2</sup>Â =Â 0.020). To conclude, results show that CNNs combined with XAI can perform lesion-symptom mapping and generate individual attribution maps, which could be a valuable feature with further method development.\n",
      "ğŸ” Recognized Entities:\n",
      " - vector (DATE)\n",
      " - CNN (ORG)\n",
      " - 821 (CARDINAL)\n",
      " - CNN (ORG)\n",
      " - FNN (ORG)\n",
      " - CNN (ORG)\n",
      " - 0.964 (CARDINAL)\n",
      " - SVR (ORG)\n",
      " - 0.875 (MONEY)\n",
      " - FNN (ORG)\n",
      " - 0.863 (CARDINAL)\n",
      " - CNN (ORG)\n",
      " - SVR (ORG)\n",
      " - 0.291 (CARDINAL)\n",
      " - CNN (ORG)\n",
      " - 0.216 (CARDINAL)\n",
      " - 0.013 (MONEY)\n",
      " - FNN (ORG)\n",
      " - 0.020 (CARDINAL)\n",
      "\n",
      "ğŸ“„ Abstract #2:\n",
      "Young-onset dementia (YOD) poses substantial societal and health care burdens. Although metabolic syndrome (MetS) is recognized as a contributor to late-onset dementia, its effect on YOD remains unclear. This study aimed to determine whether MetS and its individual components increase the risk of YOD, including all-cause dementia, Alzheimer disease (AD), and vascular dementia (VaD). We conducted a nationwide population-based cohort study using data from the Korean National Insurance Service. Individuals aged 40-60 who underwent national health check-ups in 2009 were included and followed until December 31, 2020, or age 65, whichever came first. MetS was defined according to established guidelines, incorporating measurements of waist circumference, blood pressure, fasting glucose, triglycerides, and high-density lipoprotein cholesterol. Covariates included age, sex, income level, smoking status, alcohol consumption, and comorbidities such as hypertension, diabetes, dyslipidemia, and depression. The primary outcome was incident all-cause YOD, defined as a dementia diagnosis before age 65; secondary outcomes included young-onset AD and VaD. Multivariable Cox proportional hazards models were used to estimate hazard ratios (HRs) with 95% CIs. A total of 1,979,509 participants (mean age, 49.0 years; 51.3% men; 50.7% with MetS) were included. Over an average follow-up of 7.75 years, 8,921 individuals (0.45%) developed YOD. MetS was associated with a 24% higher risk of all-cause YOD (adjusted HR 1.24, 95% CI 1.19-1.30), a 12.4% increased risk of AD (HR 1.12, 95% CI 1.03-1.22), and a 20.9% increased risk of VaD (HR 1.21, 95% CI 1.08-1.35). Significant interactions were noted with younger age (40-49 vs 50-59), female sex, drinking status, obesity, and depression. In this large Korean cohort, MetS and its individual components were significantly associated with an increased risk of YOD. These findings suggest that interventions targeting MetS may help mitigate YOD risk. However, the observational design precludes definitive causal inferences, and reliance on claims data could introduce misclassification bias. Future research using longitudinal designs and comprehensive data collection is needed to validate and expand on these associations.\n",
      "ğŸ” Recognized Entities:\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - YOD (ORG)\n",
      " - VaD (PERSON)\n",
      " - the Korean National Insurance Service (ORG)\n",
      " - 40-60 (CARDINAL)\n",
      " - 2009 (DATE)\n",
      " - December 31, 2020 (DATE)\n",
      " - age 65 (DATE)\n",
      " - first (ORDINAL)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      " - age 65 (DATE)\n",
      " - 95% (PERCENT)\n",
      " - 1,979,509 (CARDINAL)\n",
      " - 49.0 years (DATE)\n",
      " - 51.3% (PERCENT)\n",
      " - 50.7% (PERCENT)\n",
      " - MetS (ORG)\n",
      " - 7.75 years (DATE)\n",
      " - 8,921 (CARDINAL)\n",
      " - 0.45% (PERCENT)\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - 24% (PERCENT)\n",
      " - YOD (PERSON)\n",
      " - 1.24 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - 1.19-1.30 (DATE)\n",
      " - 12.4% (PERCENT)\n",
      " - 95% (PERCENT)\n",
      " - CI 1.03-1.22 (WORK_OF_ART)\n",
      " - 20.9% (PERCENT)\n",
      " - VaD (PERSON)\n",
      " - 95% (PERCENT)\n",
      " - 40-49 (CARDINAL)\n",
      " - 50-59 (DATE)\n",
      " - Korean (NORP)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      "\n",
      "ğŸ“„ Abstract #3:\n",
      "BackgroundSerum trace elements, anthropometric data, and oxidative stress markers are often altered in patients diagnosed with Alzheimer's disease (AD) or other types of dementia (OTD). However, these parameters are rarely examined together before disease onset in a single study population.ObjectiveThis nested case-control study aims to investigate anthropometric data, serum trace elements, exchangeable copper (CuEXC), and oxidative stress markers to identify early associations with the risk of AD or OTD.MethodsFrom the European Prospective Investigation into Cancer and Nutrition-Potsdam cohort (DRKS-ID: DRKS00020593), the High Fat Diet, Microbiota, and Neuroinflammation in the Progression of Alzheimer study was generated. One hundred twenty-eight individuals who developed AD or OTD were identified, approximately 15.7 years after baseline data collection, and matched for age, sex, fasting status, and season of blood sampling with 512 controls. Serum levels of manganese (Mn), iron (Fe), copper (Cu), zinc (Zn), selenium (Se), iodine (I), CuEXC, and plasma malondialdehyde (MDA) and 3-nitrotyrosine (3-NT) were analyzed.ResultsCases and non-cases did not differ in anthropometric data or oxidative stress markers. Female cases exhibited a trend of elevated serum Cu and CuEXC levels compared to female non-cases. A higher Se/Cu ratio suggested an inverse association (ORâ€‰=â€‰0.72, 95% CI: 0.56-0.92), while an increased Cu/Zn ratio was positively associated (ORâ€‰=â€‰2.1, 95% CI: 1.1-4.1) with AD or OTD incidence.ConclusionsRatios of serum trace elements, rather than individual levels, show early associations with the risk of AD or OTD while anthropometric and oxidative stress markers did not.\n",
      "ğŸ” Recognized Entities:\n",
      " - BackgroundSerum (ORG)\n",
      " - OTD (ORG)\n",
      " - CuEXC (NORP)\n",
      " - the European Prospective Investigation (ORG)\n",
      " - the High Fat Diet (ORG)\n",
      " - Microbiota (LOC)\n",
      " - Neuroinflammation (ORG)\n",
      " - the Progression of Alzheimer (ORG)\n",
      " - One hundred twenty-eight (CARDINAL)\n",
      " - OTD (ORG)\n",
      " - approximately 15.7 years (DATE)\n",
      " - 512 (CARDINAL)\n",
      " - Serum (ORG)\n",
      " - Zn (GPE)\n",
      " - Se (PERSON)\n",
      " - CuEXC (NORP)\n",
      " - plasma malondialdehyde (PERSON)\n",
      " - MDA (ORG)\n",
      " - 3 (CARDINAL)\n",
      " - 3-NT (DATE)\n",
      " - Cu (ORG)\n",
      " - CuEXC (NORP)\n",
      " - Se/Cu (PERSON)\n",
      " - 0.72 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - 0.56-0.92 (CARDINAL)\n",
      " - Cu/Zn (ORG)\n",
      " - 2.1 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - 1.1-4.1 (CARDINAL)\n",
      " - OTD (ORG)\n",
      " - OTD (ORG)\n",
      "\n",
      "ğŸ“„ Abstract #4:\n",
      "BackgroundImpairments in orientation in space, time, and person occur frequently in Alzheimer's disease (AD) dementia. Subtle changes in orientation may arise in preclinical and prodromal disease stages. Thus, assessing orientation may help identify those on a trajectory toward AD dementia.ObjectiveTo investigate how orientation, measured using a novel artificial intelligence-based paradigm, relates to AD biomarkers (amyloid and tau) in cognitively unimpaired older adults.MethodsUsing an automated chatbot, 53 cognitively unimpaired participants (74.0â€‰Â±â€‰5.5 years; 60% female) provided details about memories and relationships, recognition of historical event dates, and geographical locations. These details were then used to assess orientation to space, time, and person. For each domain separately, orientation accuracy was calculated by dividing the number of correct responses by response time. All participants underwent Pittsburgh compound-B (amyloid) and flortaucipir (tau) positron emission tomography. We analyzed the relationship between performance on the three orientation domains and retrosplenial, precuneus, neocortical, and medial temporal tau, and global amyloid.ResultsHigher retrosplenial and precuneus tau burden were associated with worse temporal orientation (Î²â€‰=â€‰-0.32, 95% confidence interval [95%CI]â€‰=â€‰[-0.59, -0.05] and Î²â€‰=â€‰-0.29, 95%CIâ€‰=â€‰[-0.57, -0.01], respectively). Spatial or social orientation were not associated with amyloid or tau.ConclusionsThese results suggest that impaired temporal orientation is related to AD pathological processes, even before the onset of overt cognitive impairment, and may infer a role for personalized assessment of orientation in early diagnosis of AD.\n",
      "ğŸ” Recognized Entities:\n",
      " - 53 (CARDINAL)\n",
      " - 74.0 (CARDINAL)\n",
      " - 5.5 years (DATE)\n",
      " - 60% (PERCENT)\n",
      " - Pittsburgh (GPE)\n",
      " - three (CARDINAL)\n",
      " - -0.32 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - -0.57 (CARDINAL)\n",
      "\n",
      "ğŸ“„ Abstract #5:\n",
      "BackgroundAlzheimer's disease (AD) is characterized by cortical atrophy, glutamatergic neuron loss, and cognitive decline. However, large-scale quantitative assessments of cellular changes during AD pathology remain scarce.ObjectiveThis study aims to integrate single-nuclei sequencing data from the Seattle Alzheimer's Disease Cortical Atlas (SEA-AD) with spatial transcriptomics to quantify cellular changes in the prefrontal cortex and temporal gyrus, regions vulnerable to AD neuropathological changes (ADNC).MethodsWe mapped differentially expressed genes (DEGs) and analyzed their interactions with pathological factors such as APOE expression and Lewy bodies. Cellular proportions were assessed, focusing on neurons, glial cells, and immune cells.ResultsRORB-expressing L4-like neurons, though vulnerable to ADNC, exhibited stable cell numbers throughout disease progression. In contrast, astrocytes displayed increased reactivity, with upregulated cytokine signaling and oxidative stress responses, suggesting a role in neuroinflammation. A reduction in synaptic maintenance pathways indicated a decline in astrocytic support functions. Microglia showed heightened immune surveillance and phagocytic activity, indicating their role in maintaining cortical homeostasis.ConclusionsThe study underscores the critical roles of glial cells, particularly astrocytes and microglia, in AD progression. These findings contribute to a better understanding of cellular dynamics and may inform therapeutic strategies targeting glial cell function in AD.\n",
      "ğŸ” Recognized Entities:\n",
      " - BackgroundAlzheimer (ORG)\n",
      " - cognitive decline (SYMPTOM)\n",
      " - APOE (ORG)\n",
      " - Lewy (ORG)\n",
      " - ADNC (ORG)\n",
      " - Microglia (PERSON)\n",
      " - ConclusionsThe (ORG)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# ğŸ”¹ 1. åŠ è½½ spaCy è‹±æ–‡æ¨¡å‹\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# ğŸ”¹ 2. æ·»åŠ  EntityRuler è‡ªå®šä¹‰å®ä½“è§„åˆ™\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# ç—‡çŠ¶å…³é”®è¯åˆ—è¡¨ï¼ˆä½ å¯ä»¥éšæ—¶æ‰©å±•ï¼‰\n",
    "symptom_keywords = [\n",
    "    \"memory loss\", \"forgetfulness\", \"cognitive decline\", \"confusion\",\n",
    "    \"disorientation\", \"language impairment\", \"attention deficit\"\n",
    "]\n",
    "\n",
    "# æ„å»ºè§„åˆ™\n",
    "patterns = [{\"label\": \"SYMPTOM\", \"pattern\": kw} for kw in symptom_keywords]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# âœ… æµ‹è¯•å•å¥æ•ˆæœï¼ˆå¯é€‰ï¼‰\n",
    "doc = nlp(\"The patient experienced memory loss and confusion over time.\")\n",
    "print(\"\\nğŸ§ª æµ‹è¯•å¥å­å®ä½“è¯†åˆ«:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\" - {ent.text} ({ent.label_})\")\n",
    "\n",
    "# ğŸ”¹ 3. è¯»å–æ‘˜è¦æ•°æ®\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\")\n",
    "\n",
    "# ğŸ”¹ 4. å¯¹å‰ 5 æ¡æ‘˜è¦æ‰§è¡Œå®ä½“è¯†åˆ«ï¼ˆå¯ä»¥æ”¹æˆå…¨éƒ¨ï¼‰\n",
    "print(\"\\nğŸ“‹ æ‰¹é‡å¤„ç†æ‘˜è¦ä¸­çš„å®ä½“è¯†åˆ«ç»“æœï¼š\")\n",
    "for i in range(5):  # æ”¹æˆ len(df) å¯ä»¥å¤„ç†å…¨éƒ¨\n",
    "    text = df.loc[i, \"Abstract\"]\n",
    "    print(f\"\\nğŸ“„ Abstract #{i+1}:\\n{text}\")\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(\"ğŸ” Recognized Entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\" - {ent.text} ({ent.label_})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ•°æ®é›†æ‹†åˆ†å®Œæˆï¼è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†å·²ä¿å­˜ã€‚\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# è¯»å–ä½ å¤„ç†è¿‡çš„æ•°æ®é›†ï¼ˆå¸¦æ ‡ç­¾ï¼‰\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts_labeled.csv\")\n",
    "\n",
    "# æ‹†åˆ†æˆè®­ç»ƒ + ä¸´æ—¶ï¼ˆdev+testï¼‰\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"MentionsSymptom\"])\n",
    "\n",
    "# å†ä» temp ä¸­æ‹†å‡º dev å’Œ testï¼ˆ50/50ï¼‰\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"MentionsSymptom\"])\n",
    "\n",
    "# ä¿å­˜ä¸‰ä¸ªæ–‡ä»¶\n",
    "train_df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_train.csv\", index=False)\n",
    "dev_df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_dev.csv\", index=False)\n",
    "test_df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_test.csv\", index=False)\n",
    "\n",
    "print(\"âœ… æ•°æ®é›†æ‹†åˆ†å®Œæˆï¼è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†å·²ä¿å­˜ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87       199\n",
      "           1       0.88      0.51      0.64        99\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.84      0.73      0.76       298\n",
      "weighted avg       0.82      0.81      0.80       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# åŠ è½½è®­ç»ƒå’Œæµ‹è¯•æ•°æ®\n",
    "train_df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_test.csv\")\n",
    "\n",
    "# æå–æ–‡æœ¬å’Œæ ‡ç­¾\n",
    "X_train = train_df[\"Abstract\"]\n",
    "y_train = train_df[\"MentionsSymptom\"]\n",
    "X_test = test_df[\"Abstract\"]\n",
    "y_test = test_df[\"MentionsSymptom\"]\n",
    "\n",
    "# TF-IDF å‘é‡åŒ–\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# é¢„æµ‹ + è¾“å‡ºç»“æœ\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87       199\n",
      "           1       0.88      0.51      0.64        99\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.84      0.73      0.76       298\n",
      "weighted avg       0.82      0.81      0.80       298\n",
      "\n",
      "ğŸ§  ROC-AUC: 0.9168\n",
      "ğŸ§© Confusion Matrix:\n",
      "[[192   7]\n",
      " [ 49  50]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# é¢„æµ‹æµ‹è¯•é›†\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "y_prob = clf.predict_proba(X_test_vec)[:, 1]\n",
    "\n",
    "# åˆ†ç±»è¯„ä¼°æŠ¥å‘Šï¼ˆåŒ…å« precision, recall, F1, accuracyï¼‰\n",
    "print(\"ğŸ“Š Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC åˆ†æ•°\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"ğŸ§  ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# è®¡ç®—æ··æ·†çŸ©é˜µ\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"ğŸ§© Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1389/1389 [00:00<00:00, 3183.85 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 298/298 [00:00<00:00, 2819.22 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 1:11:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.885906</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.972387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131512</td>\n",
       "      <td>0.963087</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.943005</td>\n",
       "      <td>0.974164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.124372</td>\n",
       "      <td>0.969799</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.952880</td>\n",
       "      <td>0.968885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131684</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.974773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12437228858470917,\n",
       " 'eval_accuracy': 0.9697986577181208,\n",
       " 'eval_precision': 0.9891304347826086,\n",
       " 'eval_recall': 0.9191919191919192,\n",
       " 'eval_f1': 0.9528795811518325,\n",
       " 'eval_roc_auc': 0.9688848281813105,\n",
       " 'eval_runtime': 32.516,\n",
       " 'eval_samples_per_second': 9.165,\n",
       " 'eval_steps_per_second': 0.584,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# 1. è¯»å–æ•°æ®\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_train.csv\")\n",
    "df_test = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_test.csv\")\n",
    "\n",
    "# 2. è½½å…¥ tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 3. é¢„å¤„ç†ï¼ˆtokenizeï¼‰\n",
    "train_dataset = Dataset.from_pandas(df[[\"Abstract\", \"MentionsSymptom\"]].rename(columns={\"Abstract\": \"text\", \"MentionsSymptom\": \"label\"}))\n",
    "test_dataset = Dataset.from_pandas(df_test[[\"Abstract\", \"MentionsSymptom\"]].rename(columns={\"Abstract\": \"text\", \"MentionsSymptom\": \"label\"}))\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "test_dataset = test_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "\n",
    "# Define label map and load model\n",
    "id2label = {0: \"NEGATIVE\", 1: \"NEUTRAL\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"NEUTRAL\": 1}\n",
    "# 4. åŠ è½½æ¨¡å‹\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "# 5. è®¾ç½®è®­ç»ƒå‚æ•°\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/text_classification_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",                      # run eval at the end of each epoch\n",
    "    save_strategy=\"epoch\", \n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    # fp16=True  # åªåœ¨æ”¯æŒ GPU çš„æ—¶å€™å¼€å¯\n",
    ")\n",
    "\n",
    "\n",
    "# 6. è‡ªå®šä¹‰æŒ‡æ ‡\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    auc = roc_auc_score(labels, pred.predictions[:, 1])\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    return {\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"precision\": report[\"1\"][\"precision\"],\n",
    "        \"recall\": report[\"1\"][\"recall\"],\n",
    "        \"f1\": report[\"1\"][\"f1-score\"],\n",
    "        \"roc_auc\": auc\n",
    "    }\n",
    "\n",
    "# 7. è®­ç»ƒæ¨¡å‹\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "# Define evaluation metric\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      No Symptom     0.9612    0.9950    0.9778       199\n",
      "Mentions Symptom     0.9891    0.9192    0.9529        99\n",
      "\n",
      "        accuracy                         0.9698       298\n",
      "       macro avg     0.9751    0.9571    0.9653       298\n",
      "    weighted avg     0.9705    0.9698    0.9695       298\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[198   1]\n",
      " [  8  91]]\n",
      "\n",
      "Overall Accuracy: 0.9697986577181208\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=[0, 1],\n",
    "    target_names=[\"No Symptom\", \"Mentions Symptom\"],\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0, 1]))\n",
    "\n",
    "print(\"\\nOverall Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "happy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
