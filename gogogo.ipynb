{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1719, 33),\n",
       " (430, 33),\n",
       " '/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/train_forgetfulness.csv',\n",
       " '/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/test_forgetfulness.csv')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# 本地路径\n",
    "data_path = \"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_disease_data.csv\"\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# 检查前几行\n",
    "df.head()\n",
    "\n",
    "# 移除不需要的列\n",
    "df_clean = df.drop(columns=[\"PatientID\", \"DoctorInCharge\"])\n",
    "\n",
    "# 设置第一阶段的目标变量，例如预测 Forgetfulness 症状\n",
    "target_variable = \"Forgetfulness\"\n",
    "\n",
    "# 拆分特征和标签\n",
    "X = df_clean.drop(columns=[target_variable])\n",
    "y = df_clean[target_variable]\n",
    "\n",
    "# 80% 训练集，20% 测试集，保持正负样本比例一致\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 合并特征和标签，保存为 DataFrame\n",
    "train_df = X_train.copy()\n",
    "train_df[target_variable] = y_train\n",
    "\n",
    "test_df = X_test.copy()\n",
    "test_df[target_variable] = y_test\n",
    "\n",
    "# 展示训练集和测试集大小\n",
    "import os\n",
    "output_dir = \"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets\"\n",
    "train_path = os.path.join(output_dir, \"train_forgetfulness.csv\")\n",
    "test_path = os.path.join(output_dir, \"test_forgetfulness.csv\")\n",
    "\n",
    "train_df.to_csv(train_path, index=False)\n",
    "test_df.to_csv(test_path, index=False)\n",
    "\n",
    "(train_df.shape, test_df.shape, train_path, test_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.85-cp311-cp311-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from biopython) (2.2.5)\n",
      "Downloading biopython-1.85-cp311-cp311-macosx_11_0_arm64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: biopython\n",
      "Successfully installed biopython-1.85\n"
     ]
    }
   ],
   "source": [
    "! pip install biopython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到文献数量: 2000\n",
      "已保存 1985 篇摘要到 csv 文件：/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "\n",
    "# 设置邮箱（NCBI 要求）\n",
    "Entrez.email = \"annabian1122@gmail.com\"\n",
    "\n",
    "# 第一步：用关键词搜索文章（比如 Alzheimer's）\n",
    "search_term = \"Alzheimer's disease AND (symptoms OR diagnosis) AND English[lang]\"\n",
    "handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=2000)\n",
    "record = Entrez.read(handle)\n",
    "handle.close()\n",
    "\n",
    "# 获取 PubMed ID 列表\n",
    "id_list = record[\"IdList\"]\n",
    "print(f\"找到文献数量: {len(id_list)}\")\n",
    "\n",
    "# 更干净的方式：用 XML 格式获取摘要数据\n",
    "handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(id_list), rettype=\"abstract\", retmode=\"xml\")\n",
    "records = Entrez.read(handle)\n",
    "handle.close()\n",
    "\n",
    "abstracts = []\n",
    "pmids = []\n",
    "\n",
    "for article in records['PubmedArticle']:\n",
    "    try:\n",
    "        abstract_text = article['MedlineCitation']['Article']['Abstract']['AbstractText']\n",
    "        # 处理成纯字符串\n",
    "        abstract_str = \" \".join(abstract_text)\n",
    "        pmid = article['MedlineCitation']['PMID']\n",
    "        abstracts.append(abstract_str)\n",
    "        pmids.append(pmid)\n",
    "    except:\n",
    "        # 没有摘要就跳过\n",
    "        continue\n",
    "\n",
    "# 保存成 DataFrame（确保只包含有摘要的文章）\n",
    "df = pd.DataFrame({\n",
    "    \"PMID\": pmids,\n",
    "    \"Abstract\": abstracts\n",
    "})\n",
    "\n",
    "\n",
    "# 保存为 CSV 文件（指定路径）\n",
    "csv_path = \"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"已保存 {len(df)} 篇摘要到 csv 文件：{csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(id_list): 2000\n",
      "len(abstracts): 1985\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(id_list): {len(id_list)}\")\n",
    "print(f\"len(abstracts): {len(abstracts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已完成关键词标签打标，保存新文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取你清洗保存的摘要数据\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\")\n",
    "\n",
    "# 自定义症状关键词列表（可扩展）\n",
    "symptom_keywords = [\n",
    "    \"memory loss\", \"forgetfulness\", \"cognitive decline\", \"confusion\",\n",
    "    \"disorientation\", \"language impairment\", \"attention deficit\", \"behavioral change\",\n",
    "    \"mild cognitive impairment\", \"cognitive symptoms\"\n",
    "]\n",
    "\n",
    "# 添加新列 MentionsSymptom：是否包含任一关键词（1/0）\n",
    "df[\"MentionsSymptom\"] = df[\"Abstract\"].apply(\n",
    "    lambda text: int(any(kw.lower() in text.lower() for kw in symptom_keywords))\n",
    ")\n",
    "\n",
    "# 保存为新文件\n",
    "df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts_labeled.csv\", index=False)\n",
    "\n",
    "print(\"✅ 已完成关键词标签打标，保存新文件。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.2.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: jinja2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# 安装 spaCy\n",
    "! pip install spacy\n",
    "\n",
    "# 下载英文小模型\n",
    "! python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 测试句子实体识别:\n",
      " - memory loss (SYMPTOM)\n",
      " - confusion (SYMPTOM)\n",
      "\n",
      "📋 批量处理摘要中的实体识别结果：\n",
      "\n",
      "📄 Abstract #1:\n",
      "Lesion-symptom mapping methods assess the relationship between lesions caused by cerebral small vessel disease and cognition, but current technology like support vector regression (SVR)) primarily provide group-level results. We propose a novel lesion-symptom mapping approach that can indicate how lesion patterns contribute to cognitive impairment on an individual level. A convolutional neural network (CNN) predicts cognitive scores and is combined with explainable artificial intelligence (XAI) to map the relation between cognition and vascular lesions. This method was evaluated primarily using real white matter hyperintensity maps of 821 memory clinic patients and simulated cognitive data, with weighted lesions and noise levels. Simulated data provided ground truth locations to assess predictive performance of the CNN and accuracy of strategic lesion identification by XAI, using an established lesion-symptom mapping method, SVR, and a simple fully connected neural network (FNN) as benchmarks. Real cognitive scores were used in a final proof-of-principle analysis. Predictive performance in simulation experiments was high for the CNN (R<sup>2</sup> = 0.964), SVR (R<sup>2</sup> = 0.875), and FNN (R<sup>2</sup> = 0.863). CNN with XAI provided patient-specific attribution maps that highlighted the ground truth locations. All methods showed similar sensitivity to noise. Using real cognitive scores, SVR (R<sup>2</sup> = 0.291) obtained a somewhat higher predictive performance than the CNN (R<sup>2</sup> = 0.216), although both methods substantially exceeded the predictive performance of total WMH volume alone (R<sup>2</sup> = 0.013). The FNN performed worse on real data (R<sup>2</sup> = 0.020). To conclude, results show that CNNs combined with XAI can perform lesion-symptom mapping and generate individual attribution maps, which could be a valuable feature with further method development.\n",
      "🔍 Recognized Entities:\n",
      " - vector (DATE)\n",
      " - CNN (ORG)\n",
      " - 821 (CARDINAL)\n",
      " - CNN (ORG)\n",
      " - FNN (ORG)\n",
      " - CNN (ORG)\n",
      " - 0.964 (CARDINAL)\n",
      " - SVR (ORG)\n",
      " - 0.875 (MONEY)\n",
      " - FNN (ORG)\n",
      " - 0.863 (CARDINAL)\n",
      " - CNN (ORG)\n",
      " - SVR (ORG)\n",
      " - 0.291 (CARDINAL)\n",
      " - CNN (ORG)\n",
      " - 0.216 (CARDINAL)\n",
      " - 0.013 (MONEY)\n",
      " - FNN (ORG)\n",
      " - 0.020 (CARDINAL)\n",
      "\n",
      "📄 Abstract #2:\n",
      "Young-onset dementia (YOD) poses substantial societal and health care burdens. Although metabolic syndrome (MetS) is recognized as a contributor to late-onset dementia, its effect on YOD remains unclear. This study aimed to determine whether MetS and its individual components increase the risk of YOD, including all-cause dementia, Alzheimer disease (AD), and vascular dementia (VaD). We conducted a nationwide population-based cohort study using data from the Korean National Insurance Service. Individuals aged 40-60 who underwent national health check-ups in 2009 were included and followed until December 31, 2020, or age 65, whichever came first. MetS was defined according to established guidelines, incorporating measurements of waist circumference, blood pressure, fasting glucose, triglycerides, and high-density lipoprotein cholesterol. Covariates included age, sex, income level, smoking status, alcohol consumption, and comorbidities such as hypertension, diabetes, dyslipidemia, and depression. The primary outcome was incident all-cause YOD, defined as a dementia diagnosis before age 65; secondary outcomes included young-onset AD and VaD. Multivariable Cox proportional hazards models were used to estimate hazard ratios (HRs) with 95% CIs. A total of 1,979,509 participants (mean age, 49.0 years; 51.3% men; 50.7% with MetS) were included. Over an average follow-up of 7.75 years, 8,921 individuals (0.45%) developed YOD. MetS was associated with a 24% higher risk of all-cause YOD (adjusted HR 1.24, 95% CI 1.19-1.30), a 12.4% increased risk of AD (HR 1.12, 95% CI 1.03-1.22), and a 20.9% increased risk of VaD (HR 1.21, 95% CI 1.08-1.35). Significant interactions were noted with younger age (40-49 vs 50-59), female sex, drinking status, obesity, and depression. In this large Korean cohort, MetS and its individual components were significantly associated with an increased risk of YOD. These findings suggest that interventions targeting MetS may help mitigate YOD risk. However, the observational design precludes definitive causal inferences, and reliance on claims data could introduce misclassification bias. Future research using longitudinal designs and comprehensive data collection is needed to validate and expand on these associations.\n",
      "🔍 Recognized Entities:\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - YOD (ORG)\n",
      " - VaD (PERSON)\n",
      " - the Korean National Insurance Service (ORG)\n",
      " - 40-60 (CARDINAL)\n",
      " - 2009 (DATE)\n",
      " - December 31, 2020 (DATE)\n",
      " - age 65 (DATE)\n",
      " - first (ORDINAL)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      " - age 65 (DATE)\n",
      " - 95% (PERCENT)\n",
      " - 1,979,509 (CARDINAL)\n",
      " - 49.0 years (DATE)\n",
      " - 51.3% (PERCENT)\n",
      " - 50.7% (PERCENT)\n",
      " - MetS (ORG)\n",
      " - 7.75 years (DATE)\n",
      " - 8,921 (CARDINAL)\n",
      " - 0.45% (PERCENT)\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - 24% (PERCENT)\n",
      " - YOD (PERSON)\n",
      " - 1.24 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - 1.19-1.30 (DATE)\n",
      " - 12.4% (PERCENT)\n",
      " - 95% (PERCENT)\n",
      " - CI 1.03-1.22 (WORK_OF_ART)\n",
      " - 20.9% (PERCENT)\n",
      " - VaD (PERSON)\n",
      " - 95% (PERCENT)\n",
      " - 40-49 (CARDINAL)\n",
      " - 50-59 (DATE)\n",
      " - Korean (NORP)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      " - MetS (ORG)\n",
      " - YOD (PERSON)\n",
      "\n",
      "📄 Abstract #3:\n",
      "BackgroundSerum trace elements, anthropometric data, and oxidative stress markers are often altered in patients diagnosed with Alzheimer's disease (AD) or other types of dementia (OTD). However, these parameters are rarely examined together before disease onset in a single study population.ObjectiveThis nested case-control study aims to investigate anthropometric data, serum trace elements, exchangeable copper (CuEXC), and oxidative stress markers to identify early associations with the risk of AD or OTD.MethodsFrom the European Prospective Investigation into Cancer and Nutrition-Potsdam cohort (DRKS-ID: DRKS00020593), the High Fat Diet, Microbiota, and Neuroinflammation in the Progression of Alzheimer study was generated. One hundred twenty-eight individuals who developed AD or OTD were identified, approximately 15.7 years after baseline data collection, and matched for age, sex, fasting status, and season of blood sampling with 512 controls. Serum levels of manganese (Mn), iron (Fe), copper (Cu), zinc (Zn), selenium (Se), iodine (I), CuEXC, and plasma malondialdehyde (MDA) and 3-nitrotyrosine (3-NT) were analyzed.ResultsCases and non-cases did not differ in anthropometric data or oxidative stress markers. Female cases exhibited a trend of elevated serum Cu and CuEXC levels compared to female non-cases. A higher Se/Cu ratio suggested an inverse association (OR = 0.72, 95% CI: 0.56-0.92), while an increased Cu/Zn ratio was positively associated (OR = 2.1, 95% CI: 1.1-4.1) with AD or OTD incidence.ConclusionsRatios of serum trace elements, rather than individual levels, show early associations with the risk of AD or OTD while anthropometric and oxidative stress markers did not.\n",
      "🔍 Recognized Entities:\n",
      " - BackgroundSerum (ORG)\n",
      " - OTD (ORG)\n",
      " - CuEXC (NORP)\n",
      " - the European Prospective Investigation (ORG)\n",
      " - the High Fat Diet (ORG)\n",
      " - Microbiota (LOC)\n",
      " - Neuroinflammation (ORG)\n",
      " - the Progression of Alzheimer (ORG)\n",
      " - One hundred twenty-eight (CARDINAL)\n",
      " - OTD (ORG)\n",
      " - approximately 15.7 years (DATE)\n",
      " - 512 (CARDINAL)\n",
      " - Serum (ORG)\n",
      " - Zn (GPE)\n",
      " - Se (PERSON)\n",
      " - CuEXC (NORP)\n",
      " - plasma malondialdehyde (PERSON)\n",
      " - MDA (ORG)\n",
      " - 3 (CARDINAL)\n",
      " - 3-NT (DATE)\n",
      " - Cu (ORG)\n",
      " - CuEXC (NORP)\n",
      " - Se/Cu (PERSON)\n",
      " - 0.72 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - 0.56-0.92 (CARDINAL)\n",
      " - Cu/Zn (ORG)\n",
      " - 2.1 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - 1.1-4.1 (CARDINAL)\n",
      " - OTD (ORG)\n",
      " - OTD (ORG)\n",
      "\n",
      "📄 Abstract #4:\n",
      "BackgroundImpairments in orientation in space, time, and person occur frequently in Alzheimer's disease (AD) dementia. Subtle changes in orientation may arise in preclinical and prodromal disease stages. Thus, assessing orientation may help identify those on a trajectory toward AD dementia.ObjectiveTo investigate how orientation, measured using a novel artificial intelligence-based paradigm, relates to AD biomarkers (amyloid and tau) in cognitively unimpaired older adults.MethodsUsing an automated chatbot, 53 cognitively unimpaired participants (74.0 ± 5.5 years; 60% female) provided details about memories and relationships, recognition of historical event dates, and geographical locations. These details were then used to assess orientation to space, time, and person. For each domain separately, orientation accuracy was calculated by dividing the number of correct responses by response time. All participants underwent Pittsburgh compound-B (amyloid) and flortaucipir (tau) positron emission tomography. We analyzed the relationship between performance on the three orientation domains and retrosplenial, precuneus, neocortical, and medial temporal tau, and global amyloid.ResultsHigher retrosplenial and precuneus tau burden were associated with worse temporal orientation (β = -0.32, 95% confidence interval [95%CI] = [-0.59, -0.05] and β = -0.29, 95%CI = [-0.57, -0.01], respectively). Spatial or social orientation were not associated with amyloid or tau.ConclusionsThese results suggest that impaired temporal orientation is related to AD pathological processes, even before the onset of overt cognitive impairment, and may infer a role for personalized assessment of orientation in early diagnosis of AD.\n",
      "🔍 Recognized Entities:\n",
      " - 53 (CARDINAL)\n",
      " - 74.0 (CARDINAL)\n",
      " - 5.5 years (DATE)\n",
      " - 60% (PERCENT)\n",
      " - Pittsburgh (GPE)\n",
      " - three (CARDINAL)\n",
      " - -0.32 (CARDINAL)\n",
      " - 95% (PERCENT)\n",
      " - -0.57 (CARDINAL)\n",
      "\n",
      "📄 Abstract #5:\n",
      "BackgroundAlzheimer's disease (AD) is characterized by cortical atrophy, glutamatergic neuron loss, and cognitive decline. However, large-scale quantitative assessments of cellular changes during AD pathology remain scarce.ObjectiveThis study aims to integrate single-nuclei sequencing data from the Seattle Alzheimer's Disease Cortical Atlas (SEA-AD) with spatial transcriptomics to quantify cellular changes in the prefrontal cortex and temporal gyrus, regions vulnerable to AD neuropathological changes (ADNC).MethodsWe mapped differentially expressed genes (DEGs) and analyzed their interactions with pathological factors such as APOE expression and Lewy bodies. Cellular proportions were assessed, focusing on neurons, glial cells, and immune cells.ResultsRORB-expressing L4-like neurons, though vulnerable to ADNC, exhibited stable cell numbers throughout disease progression. In contrast, astrocytes displayed increased reactivity, with upregulated cytokine signaling and oxidative stress responses, suggesting a role in neuroinflammation. A reduction in synaptic maintenance pathways indicated a decline in astrocytic support functions. Microglia showed heightened immune surveillance and phagocytic activity, indicating their role in maintaining cortical homeostasis.ConclusionsThe study underscores the critical roles of glial cells, particularly astrocytes and microglia, in AD progression. These findings contribute to a better understanding of cellular dynamics and may inform therapeutic strategies targeting glial cell function in AD.\n",
      "🔍 Recognized Entities:\n",
      " - BackgroundAlzheimer (ORG)\n",
      " - cognitive decline (SYMPTOM)\n",
      " - APOE (ORG)\n",
      " - Lewy (ORG)\n",
      " - ADNC (ORG)\n",
      " - Microglia (PERSON)\n",
      " - ConclusionsThe (ORG)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# 🔹 1. 加载 spaCy 英文模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 🔹 2. 添加 EntityRuler 自定义实体规则\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# 症状关键词列表（你可以随时扩展）\n",
    "symptom_keywords = [\n",
    "    \"memory loss\", \"forgetfulness\", \"cognitive decline\", \"confusion\",\n",
    "    \"disorientation\", \"language impairment\", \"attention deficit\"\n",
    "]\n",
    "\n",
    "# 构建规则\n",
    "patterns = [{\"label\": \"SYMPTOM\", \"pattern\": kw} for kw in symptom_keywords]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# ✅ 测试单句效果（可选）\n",
    "doc = nlp(\"The patient experienced memory loss and confusion over time.\")\n",
    "print(\"\\n🧪 测试句子实体识别:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\" - {ent.text} ({ent.label_})\")\n",
    "\n",
    "# 🔹 3. 读取摘要数据\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts.csv\")\n",
    "\n",
    "# 🔹 4. 对前 5 条摘要执行实体识别（可以改成全部）\n",
    "print(\"\\n📋 批量处理摘要中的实体识别结果：\")\n",
    "for i in range(5):  # 改成 len(df) 可以处理全部\n",
    "    text = df.loc[i, \"Abstract\"]\n",
    "    print(f\"\\n📄 Abstract #{i+1}:\\n{text}\")\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    print(\"🔍 Recognized Entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\" - {ent.text} ({ent.label_})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 数据集拆分完成！训练/验证/测试集已保存。\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取你处理过的数据集（带标签）\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/alzheimers_abstracts_labeled.csv\")\n",
    "\n",
    "# 拆分成训练 + 临时（dev+test）\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"MentionsSymptom\"])\n",
    "\n",
    "# 再从 temp 中拆出 dev 和 test（50/50）\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"MentionsSymptom\"])\n",
    "\n",
    "# 保存三个文件\n",
    "train_df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_train.csv\", index=False)\n",
    "dev_df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_dev.csv\", index=False)\n",
    "test_df.to_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_test.csv\", index=False)\n",
    "\n",
    "print(\"✅ 数据集拆分完成！训练/验证/测试集已保存。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87       199\n",
      "           1       0.88      0.51      0.64        99\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.84      0.73      0.76       298\n",
      "weighted avg       0.82      0.81      0.80       298\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# 加载训练和测试数据\n",
    "train_df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_train.csv\")\n",
    "test_df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_test.csv\")\n",
    "\n",
    "# 提取文本和标签\n",
    "X_train = train_df[\"Abstract\"]\n",
    "y_train = train_df[\"MentionsSymptom\"]\n",
    "X_test = test_df[\"Abstract\"]\n",
    "y_test = test_df[\"MentionsSymptom\"]\n",
    "\n",
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# 训练逻辑回归模型\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# 预测 + 输出结果\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87       199\n",
      "           1       0.88      0.51      0.64        99\n",
      "\n",
      "    accuracy                           0.81       298\n",
      "   macro avg       0.84      0.73      0.76       298\n",
      "weighted avg       0.82      0.81      0.80       298\n",
      "\n",
      "🧠 ROC-AUC: 0.9168\n",
      "🧩 Confusion Matrix:\n",
      "[[192   7]\n",
      " [ 49  50]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "y_prob = clf.predict_proba(X_test_vec)[:, 1]\n",
    "\n",
    "# 分类评估报告（包含 precision, recall, F1, accuracy）\n",
    "print(\"📊 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ROC-AUC 分数\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "print(f\"🧠 ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 计算混淆矩阵\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"🧩 Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/zhengfeibian/anaconda3/envs/happy/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1389/1389 [00:00<00:00, 3183.85 examples/s]\n",
      "Map: 100%|██████████| 298/298 [00:00<00:00, 2819.22 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='348' max='348' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [348/348 1:11:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.885906</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.841121</td>\n",
       "      <td>0.972387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131512</td>\n",
       "      <td>0.963087</td>\n",
       "      <td>0.968085</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.943005</td>\n",
       "      <td>0.974164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.124372</td>\n",
       "      <td>0.969799</td>\n",
       "      <td>0.989130</td>\n",
       "      <td>0.919192</td>\n",
       "      <td>0.952880</td>\n",
       "      <td>0.968885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131684</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.929293</td>\n",
       "      <td>0.948454</td>\n",
       "      <td>0.974773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.12437228858470917,\n",
       " 'eval_accuracy': 0.9697986577181208,\n",
       " 'eval_precision': 0.9891304347826086,\n",
       " 'eval_recall': 0.9191919191919192,\n",
       " 'eval_f1': 0.9528795811518325,\n",
       " 'eval_roc_auc': 0.9688848281813105,\n",
       " 'eval_runtime': 32.516,\n",
       " 'eval_samples_per_second': 9.165,\n",
       " 'eval_steps_per_second': 0.584,\n",
       " 'epoch': 4.0}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# 1. 读取数据\n",
    "df = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_train.csv\")\n",
    "df_test = pd.read_csv(\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/Abstracts_test.csv\")\n",
    "\n",
    "# 2. 载入 tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# 3. 预处理（tokenize）\n",
    "train_dataset = Dataset.from_pandas(df[[\"Abstract\", \"MentionsSymptom\"]].rename(columns={\"Abstract\": \"text\", \"MentionsSymptom\": \"label\"}))\n",
    "test_dataset = Dataset.from_pandas(df_test[[\"Abstract\", \"MentionsSymptom\"]].rename(columns={\"Abstract\": \"text\", \"MentionsSymptom\": \"label\"}))\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "test_dataset = test_dataset.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "\n",
    "# Define label map and load model\n",
    "id2label = {0: \"NEGATIVE\", 1: \"NEUTRAL\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"NEUTRAL\": 1}\n",
    "# 4. 加载模型\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "# 5. 设置训练参数\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Users/zhengfeibian/Desktop/5630final/MyOwnChooseDataSets/text_classification_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",                      # run eval at the end of each epoch\n",
    "    save_strategy=\"epoch\", \n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    # fp16=True  # 只在支持 GPU 的时候开启\n",
    ")\n",
    "\n",
    "\n",
    "# 6. 自定义指标\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    auc = roc_auc_score(labels, pred.predictions[:, 1])\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    return {\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"precision\": report[\"1\"][\"precision\"],\n",
    "        \"recall\": report[\"1\"][\"recall\"],\n",
    "        \"f1\": report[\"1\"][\"f1-score\"],\n",
    "        \"roc_auc\": auc\n",
    "    }\n",
    "\n",
    "# 7. 训练模型\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.51.3\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "y_true = predictions.label_ids\n",
    "# Define evaluation metric\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      No Symptom     0.9612    0.9950    0.9778       199\n",
      "Mentions Symptom     0.9891    0.9192    0.9529        99\n",
      "\n",
      "        accuracy                         0.9698       298\n",
      "       macro avg     0.9751    0.9571    0.9653       298\n",
      "    weighted avg     0.9705    0.9698    0.9695       298\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[198   1]\n",
      " [  8  91]]\n",
      "\n",
      "Overall Accuracy: 0.9697986577181208\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=[0, 1],\n",
    "    target_names=[\"No Symptom\", \"Mentions Symptom\"],\n",
    "    digits=4\n",
    ")\n",
    "print(report)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred, labels=[0, 1]))\n",
    "\n",
    "print(\"\\nOverall Accuracy:\", accuracy_score(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "happy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
