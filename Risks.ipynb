{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRrLXFBM73M0",
    "outputId": "5a36efb2-b4e9-4231-9f12-03bb504eb17d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read your cleaned and saved abstracts dataset\n",
    "df = pd.read_csv(\"/content/alzheimers_abstracts.csv\")\n",
    "\n",
    "# Define a list of risk factor keywords\n",
    "risk_keywords = [\n",
    "    \"air pollution\", \"PM2.5\", \"particulate matter\", \"environmental exposure\",\n",
    "    \"toxins\", \"neuroinflammation\", \"smoking\", \"hypertension\", \"cholesterol\",\n",
    "    \"diet\", \"sleep quality\", \"obesity\", \"pesticides\"\n",
    "]\n",
    "\n",
    "# Add a new column 'Factors': mark 1 if any keyword is found in the abstract, else 0\n",
    "df[\"Factors\"] = df[\"Abstract\"].apply(\n",
    "    lambda text: int(any(kw.lower() in text.lower() for kw in risk_keywords))\n",
    ")\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"alzheimers_abstracts_risk.csv\", index=False)\n",
    "\n",
    "print(\"Successfully completed keyword tagging for risk factors and saved the new file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gjf6zMfw73M2",
    "outputId": "4804afbe-b061-4294-abd0-8a93a946909d"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Read your processed dataset (already labeled with 'Factors')\n",
    "df = pd.read_csv(\"/content/alzheimers_abstracts_risk.csv\")\n",
    "\n",
    "# Split the dataset into Training set + Temporary set (dev + test)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"Factors\"])\n",
    "\n",
    "# Further split the Temporary set into Dev and Test sets (50% each)\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"Factors\"])\n",
    "\n",
    "# Save the resulting datasets to new CSV files\n",
    "train_df.to_csv(\"risk_train.csv\", index=False)\n",
    "dev_df.to_csv(\"risk_dev.csv\", index=False)\n",
    "test_df.to_csv(\"risk_test.csv\", index=False)\n",
    "\n",
    "print(\"Train/Dev/Test sets have been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCm2CuI673M3",
    "outputId": "a30e7d96-a417-4485-b3bb-74bec98785d7"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add an EntityRuler to insert custom entity recognition patterns before the built-in NER\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define a list of risk factor keywords\n",
    "risk_keywords = [\n",
    "    \"air pollution\", \"PM2.5\", \"particulate matter\", \"environmental exposure\",\n",
    "    \"toxins\", \"neuroinflammation\", \"smoking\", \"hypertension\", \"cholesterol\",\n",
    "    \"diet\", \"sleep quality\", \"obesity\", \"pesticides\"\n",
    "]\n",
    "\n",
    "# Build custom patterns with label 'RISK'\n",
    "patterns = [{\"label\": \"RISK\", \"pattern\": kw} for kw in risk_keywords]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "# Test entity recognition on a single simple sentence\n",
    "doc = nlp(\"The patient experienced air pollution.\")\n",
    "print(\"\\nTesting entity recognition on a sample sentence:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\" - {ent.text} ({ent.label_})\")\n",
    "\n",
    "# Read the abstracts dataset\n",
    "df = pd.read_csv(\"/content/alzheimers_abstracts.csv\")\n",
    "\n",
    "# Perform entity recognition on the first 5 abstracts\n",
    "print(\"\\nBatch processing entity recognition results:\")\n",
    "for i in range(5):\n",
    "    text = df.loc[i, \"Abstract\"]\n",
    "    print(f\"\\nAbstract #{i+1}:\\n{text}\")\n",
    "\n",
    "    doc = nlp(text) # Apply NLP pipeline\n",
    "\n",
    "    print(\"Recognized Entities:\")\n",
    "    for ent in doc.ents:\n",
    "        print(f\" - {ent.text} ({ent.label_})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xqyWPQxl73M3",
    "outputId": "fa6a9d74-6528-4342-d801-3666f60082e2"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# Load the training and testing datasets\n",
    "train_df = pd.read_csv(\"/content/risk_train.csv\")\n",
    "test_df = pd.read_csv(\"/content/risk_test.csv\")\n",
    "\n",
    "# Extract features (text) and labels\n",
    "X_train2 = train_df[\"Abstract\"]\n",
    "y_train2 = train_df[\"Factors\"]\n",
    "X_test2 = test_df[\"Abstract\"]\n",
    "y_test2 = test_df[\"Factors\"]\n",
    "\n",
    "# Apply TF-IDF vectorization to the text\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec2 = vectorizer.fit_transform(X_train2)\n",
    "X_test_vec2 = vectorizer.transform(X_test2)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "clf.fit(X_train_vec2, y_train2)\n",
    "\n",
    "# Predict and print the classification report\n",
    "y_pred2 = clf.predict(X_test_vec2)\n",
    "print(classification_report(y_test2, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZP7rcvew73M4",
    "outputId": "90b19ad1-7ac3-4db8-d43d-df38d14bb107"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred2 = clf.predict(X_test_vec2) # Predicted class labels (0 or 1)\n",
    "y_prob2 = clf.predict_proba(X_test_vec2)[:, 1]\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test2, y_pred2))\n",
    "\n",
    "# Calculate and print ROC-AUC score\n",
    "auc = roc_auc_score(y_test2, y_prob2)\n",
    "print(f\"ROC-AUC: {auc:.4f}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute and print the confusion matrix\n",
    "cm = confusion_matrix(y_test2, y_pred2)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd7_7ghL-Za7",
    "outputId": "4184aced-f745-4ac3-8f49-7fec54ef46fa"
   },
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 839,
     "referenced_widgets": [
      "ba6eb3e3224b4b91bdcfcb1255d00bca",
      "cfb6ed543e3a4b4c97839910e242d885",
      "e7ee6cc47b6348d483d7240dfeb3c280",
      "55f233320fef43bc8453f318d3acae83",
      "762878d6713142588f8333c545e8a3b5",
      "4c6671c3899e4c0e983fbede55f0775b",
      "04a2f2ee318e43f2a0e784cd765a218c",
      "d1f1e066e35f49f39a36ad0fd58a4a1c",
      "5c306d996fbd435dab22e448860d5e96",
      "8ac11aa5a7334da68aa22f9d6251ff99",
      "da5235323538486eb598444a26abbac6",
      "a40d4c9cb4314a13aa09d0161f8698ae",
      "2cfa9018427646dbbcc5f484a5a79c14",
      "7e99e8b008fd4cacbc005ada14b84404",
      "40516da894794a0c86d9b15d47522511",
      "34baf943dcdc4e5bb26cfb6767ea7c00",
      "e5353bf8e8234a1f87280b5966b6bd40",
      "302066f63c74453fb195bbafbadfbc53",
      "751fa83abce946a6a1022c3cc49fab13",
      "303ed7c90c97440b9bd64ca300de496f",
      "744a5dc2be46484fb90027bcbf4e8732",
      "93dfb35b2c0542b2b011ebc24758ed86",
      "189e7efd324b424b8f2adc3d0579375f",
      "99ab4cd6dea54722b421010d1653e9af",
      "bb64a04de83d4df0bca892f6b0c44e55",
      "00dfdfca08e142dc996f972ac636efcd",
      "e993037ca3dc458fa11c36e783224a98",
      "656affe8ffc142bc898015f72d0c06d4",
      "b4013da1a6934330b5b1337ff58571da",
      "d2388995f5514d89978805d1824a7e59",
      "0f351638291744c48ba95ccc37a1b3ec",
      "607f68a641f9427ca7a27e051d8a680d",
      "5fdabdcad7cb48b1b4b4e4995d8ed122",
      "73a1469e865b4c69a6f9641cf2733635",
      "0336d4159e5045ad9b5e442cf8d1df9a",
      "9c33d51a1d514293978be6a9427be331",
      "12f2873cc7104f0f9cc556ca5a224b1c",
      "c8a841d6d9354067a82032b61f3ba5b3",
      "2750971963004b38bbfaa27e53fef45e",
      "88cbba7972ef489b9d57823c4d79fb20",
      "8b1be1efd0f2488888faffb11be6d058",
      "446ac9bc499249f5a4e85a166b8e3ece",
      "eadc81fda2e843fcab20a09868e4bab3",
      "efbc04ad1a754643957ac8de20e0725c",
      "48e332f30f354db1bd876fafa423d4a9",
      "327025596a834b1296aec66e2e3eea81",
      "390f7bdba2e44ddb899d000c83e75afb",
      "007309b35a8a44909be585cc78da94f8",
      "e2afd4430a514f42b0db4b436e8f2770",
      "7e1172c6719b41f8ae544ec34dae4d8d",
      "15958bcef18e4d09bd5fdb9a50d70469",
      "bdd6a956d53b411caa3b46c5d6692221",
      "a4266e1786a84b51be09cacbf2ee9228",
      "bda3afa555ba4e77b134ac1bd4fb4408",
      "9fcc65d4fd6a40e68bcbd180134bdb1a",
      "5f253e18a6bb4a10ab344a4999004942",
      "27135591799a4db6bd618855346f635d",
      "c2b70006ae2a44e2bd5f94f68afe6223",
      "b54dc666191944348530271199f2da75",
      "f824e8e23882438680a37f420aab41d6",
      "e3de2dd001604812b9a7112fbd08c3b4",
      "cd32df17092f47a293fde9d603e9bafd",
      "93585b42e57c425e9eb824401cab2800",
      "ef4aa38a6b77443f9fdc42634eec4f76",
      "fbc0061af5264afab4f04f82dd1d1489",
      "0e9669c74fe54c328fcfb1f673e9f01c",
      "eb87baf460ec4dfdba21079ffd14aca1",
      "8e3064e5f1104fd692ab036cbace0529",
      "c579d16d7a94498293d907c9043eb5e1",
      "366d4eaf831e48f491e99a00ed1b987d",
      "924de771158442cdaee031b66802a47d",
      "f98190eadcbe418dbbfd876c534e6940",
      "0762fb3e4bf24b78a2a85074cf3325dd",
      "4b9e132e43434b86a1ada051ea6ded80",
      "d01e2564f57547fb8a82a0cd47621b41",
      "3e5bd245e97640f4be545c840e44c803",
      "e2ace25a7ca546128012ade4164700b6"
     ]
    },
    "id": "9iMSPOpe73M4",
    "outputId": "b7d9727c-7a2f-4838-a084-22f413f503bf"
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Load the train and test datasets\n",
    "df = pd.read_csv(\"/content/risk_train.csv\")\n",
    "df_test = pd.read_csv(\"/content/risk_test.csv\")\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Preprocessing: prepare Huggingface Dataset objects\n",
    "# Rename columns to match Huggingface expected names (\"text\", \"label\")\n",
    "train_dataset2 = Dataset.from_pandas(df[[\"Abstract\", \"Factors\"]].rename(columns={\"Abstract\": \"text\", \"Factors\": \"label\"}))\n",
    "test_dataset2 = Dataset.from_pandas(df_test[[\"Abstract\", \"Factors\"]].rename(columns={\"Abstract\": \"text\", \"Factors\": \"label\"}))\n",
    "# Apply tokenization\n",
    "train_dataset2 = train_dataset2.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "test_dataset2 = test_dataset2.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\"), batched=True)\n",
    "\n",
    "# Define label map and load model\n",
    "id2label = {0: \"No risk\", 1: \"Mentions risk\"}\n",
    "label2id = {\"No risk\": 0, \"Mentions risk\": 1}\n",
    "\n",
    "# load model\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "# Set training parameters\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"text_classification_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",                      # run eval at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True  # using a GPU with FP16 support with Colab\n",
    ")\n",
    "\n",
    "\n",
    "# Define custom evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    auc = roc_auc_score(labels, pred.predictions[:, 1])\n",
    "    report = classification_report(labels, preds, output_dict=True)\n",
    "    return {\n",
    "        \"accuracy\": report[\"accuracy\"],\n",
    "        \"precision\": report[\"1\"][\"precision\"],\n",
    "        \"recall\": report[\"1\"][\"recall\"],\n",
    "        \"f1\": report[\"1\"][\"f1-score\"],\n",
    "        \"roc_auc\": auc\n",
    "    }\n",
    "\n",
    "# Initialize Trainer and train model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset2,\n",
    "    eval_dataset=test_dataset2,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train() # Start model training\n",
    "trainer.evaluate() # Evaluate model on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abEAEFOT73M4",
    "outputId": "cac8a3a4-17dd-4df2-aaae-3389e9382e6d"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v90nFn0g_i5i",
    "outputId": "3326f340-f7d6-4e6e-e74c-f9329d1cdae8"
   },
   "outputs": [],
   "source": [
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bc06956f47b7492f8126968fd5f56a2e",
      "cc806a6d04aa45958ca302da9d9f4ff9",
      "b641afb039174e09bc665c78ceb83547",
      "b8b99815ecaf4904a3be09a928e88f5d",
      "1a73158d6f4641869c7e82c61e161ff2",
      "ed2b83f9e3ac4bca86aaaa54eb5dcb5e",
      "aea57330e1ac44c5838595887e6642f4",
      "45e00e51dfa447388f2d010f82bfe49c",
      "98a299be071b41d69c48972fafa1ff33",
      "639e917e1d89444e93c9b7f993bd5811",
      "04294671013e4703bd4ed0810777b583"
     ]
    },
    "id": "vkOHJj0M73M4",
    "outputId": "b7cb4bcd-aae5-4936-ff29-23253c857431"
   },
   "outputs": [],
   "source": [
    "# Define evaluation metric\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c0AxhpT473M5"
   },
   "outputs": [],
   "source": [
    "# Custom function to compute metrics during evaluation\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions2, labels = eval_pred\n",
    "    preds2 = np.argmax(predictions2, axis=1)\n",
    "    return accuracy.compute(predictions=preds2, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "4ZmXSYgs73M5",
    "outputId": "46ef47f2-f4db-4ae1-f2c1-2d731881e28d"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test dataset using the trained model\n",
    "predictions2 = trainer.predict(test_dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNoLDYWy73M5"
   },
   "outputs": [],
   "source": [
    "# Extract predicted labels and true labels\n",
    "y_pred2 = np.argmax(predictions2.predictions, axis=1)\n",
    "y_true2 = predictions2.label_ids\n",
    "# Define evaluation metric\n",
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jVb3MV5a73M5",
    "outputId": "055a50c8-48cb-42f8-d820-6b725deb718e"
   },
   "outputs": [],
   "source": [
    "# Print detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "report = classification_report(\n",
    "    y_true2, y_pred2,\n",
    "    labels=[0, 1], # Define the order of labels\n",
    "    target_names=[\"No risk\", \"Mentions risk\"], # Define label names\n",
    "    digits=4 # Display results with 4 decimal places\n",
    ")\n",
    "print(report)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# Print confusion matrix and overall accuracy\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_true2, y_pred2, labels=[0, 1]))\n",
    "\n",
    "print(\"\\nOverall Accuracy:\", accuracy_score(y_true2, y_pred2))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
